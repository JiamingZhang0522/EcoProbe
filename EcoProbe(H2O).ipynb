{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Load the data\n",
    "data_path = 'Total.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Independent variables (features)\n",
    "features = ['WVR', 'CHLA_RESULT', 'PIP_PT', 'TIP_PT', 'EVI']\n",
    "\n",
    "# Dependent variables (targets)\n",
    "targets = ['AMMONIA_N_RESULT', 'CHLORIDE_RESULT', 'COND_RESULT', 'DOC_RESULT',\n",
    "           'NITRATE_NITRITE_N_RESULT', 'NTL_RESULT', 'PH_RESULT', 'PTL_RESULT',\n",
    "           'SULFATE_RESULT', 'TKN_RESULT', 'TURB_RESULT']\n",
    "\n",
    "# Prepare data by dropping rows with NaN values in features or targets\n",
    "filtered_data = data.dropna(subset=features + targets)\n",
    "\n",
    "# Extract features and targets\n",
    "X = filtered_data[features].values\n",
    "y = filtered_data[targets].values\n",
    "\n",
    "# Store results for each target\n",
    "for target_idx, target in enumerate(targets):\n",
    "    print(f\"Training model for {target}...\\n\")\n",
    "    \n",
    "    # Create a pipeline with scaling, polynomial features, and Ridge regression model\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Standardize the features\n",
    "        ('poly', PolynomialFeatures(degree=3, include_bias=False)),  # Polynomial features (degree 3)\n",
    "        ('model', Ridge(alpha=1.0))  # Ridge regression for regularization\n",
    "    ])\n",
    "    \n",
    "    # Set up k-fold cross-validation (e.g., 5 folds)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Cross-validation: Evaluate the model using R^2 score\n",
    "    cv_scores = cross_val_score(pipeline, X, y[:, target_idx], cv=kf, scoring='r2')\n",
    "    \n",
    "    # Train the model for the current target variable (on the whole dataset)\n",
    "    pipeline.fit(X, y[:, target_idx])  # Fit the model for the current target\n",
    "    \n",
    "    # Get predictions for the full dataset\n",
    "    y_pred = pipeline.predict(X)\n",
    "    \n",
    "    # Calculate mean squared error for the current target variable\n",
    "    mse = mean_squared_error(y[:, target_idx], y_pred)  # Compare to the current target (not y_all)\n",
    "    \n",
    "    # Calculate R-squared for the current target variable\n",
    "    r2 = r2_score(y[:, target_idx], y_pred)  # Compare to the current target (not y_all)\n",
    "    \n",
    "    # Display results for the current target\n",
    "    print(f\"Results for {target}:\")\n",
    "    print(f\"  Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    print(\"\\nModel trained for\", target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Load the data\n",
    "data_path = 'Total.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Independent variables (features)\n",
    "features = ['WVR', 'CHLA_RESULT', 'PIP_PT', 'TIP_PT', 'EVI']\n",
    "\n",
    "# Dependent variables (targets)\n",
    "targets = ['AMMONIA_N_RESULT', 'CHLORIDE_RESULT', 'COND_RESULT', 'DOC_RESULT',\n",
    "           'NITRATE_NITRITE_N_RESULT', 'NTL_RESULT', 'PH_RESULT', 'PTL_RESULT',\n",
    "           'SULFATE_RESULT', 'TKN_RESULT', 'TURB_RESULT']\n",
    "\n",
    "# Prepare data by dropping rows with NaN values in features or targets\n",
    "filtered_data = data.dropna(subset=features + targets)\n",
    "\n",
    "# Extract features and targets\n",
    "X = filtered_data[features].values\n",
    "y = filtered_data[targets].values\n",
    "\n",
    "# Create a PDF to save the plots\n",
    "pdf_filename = 'model_plots.pdf'\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "\n",
    "    # Store results for each target\n",
    "    for target_idx, target in enumerate(targets):\n",
    "        print(f\"Training model for {target}...\\n\")\n",
    "        \n",
    "        # Create a pipeline with scaling, polynomial features, and Ridge regression model\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),  # Standardize the features\n",
    "            ('poly', PolynomialFeatures(degree=3, include_bias=False)),  # Polynomial features (degree 3)\n",
    "            ('model', Ridge(alpha=1.0))  # Ridge regression for regularization\n",
    "        ])\n",
    "        \n",
    "        # Set up k-fold cross-validation (5 folds)\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Store the R² scores for each fold\n",
    "        fold_r2_scores = []\n",
    "        \n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx, target_idx], y[test_idx, target_idx]\n",
    "            \n",
    "            # Train the model on the training data\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict on the test data\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            \n",
    "            # Calculate R² for this fold\n",
    "            fold_r2 = r2_score(y_test, y_pred)\n",
    "            fold_r2_scores.append(fold_r2)\n",
    "        \n",
    "        # Train the model for the current target variable on the full dataset\n",
    "        pipeline.fit(X, y[:, target_idx])  # Fit the model for the current target\n",
    "        \n",
    "        # Get predictions for the full dataset\n",
    "        y_pred = pipeline.predict(X)\n",
    "        \n",
    "        # Calculate mean squared error for the current target variable\n",
    "        mse = mean_squared_error(y[:, target_idx], y_pred)  # Compare to the current target (not y_all)\n",
    "        \n",
    "        # Calculate R-squared for the current target variable\n",
    "        r2 = r2_score(y[:, target_idx], y_pred)  # Compare to the current target (not y_all)\n",
    "        \n",
    "        # Calculate F-statistic and p-value for the model\n",
    "        n = X.shape[0]  # Sample size\n",
    "        p = X.shape[1] + 1  # Number of predictors including polynomial terms\n",
    "        rss = np.sum((y[:, target_idx] - y_pred) ** 2)\n",
    "        tss = np.sum((y[:, target_idx] - np.mean(y[:, target_idx])) ** 2)\n",
    "        f_stat = (tss - rss) / p / (rss / (n - p - 1))\n",
    "        p_value = 1 - stats.f.cdf(f_stat, p, n - p - 1)\n",
    "        \n",
    "        # Create a new figure for the plot\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Plot actual vs predicted\n",
    "        ax.scatter(y[:, target_idx], y_pred, color='blue', alpha=0.6, label='Actual vs Predicted')\n",
    "        \n",
    "        # Plot a line for perfect predictions (where predicted = actual)\n",
    "        ax.plot([min(y[:, target_idx]), max(y[:, target_idx])], \n",
    "                [min(y[:, target_idx]), max(y[:, target_idx])], color='red', linestyle='--', label='Perfect Prediction')\n",
    "        \n",
    "        # Display the R² value and p-value on the plot\n",
    "        ax.text(0.1, 0.9, f'R² = {r2:.2f}', transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
    "        \n",
    "        # Check if p-value is less than 0.001\n",
    "        p_value_text = f'P-value = {p_value:.4f}'\n",
    "        if p_value < 0.001:\n",
    "            p_value_text = 'P-value < 0.001'\n",
    "        \n",
    "        ax.text(0.1, 0.85, p_value_text, transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
    "        \n",
    "        # Add labels and title\n",
    "        ax.set_xlabel('Actual Values')\n",
    "        ax.set_ylabel('Predicted Values')\n",
    "        ax.set_title(f'{target} - Actual vs Predicted')\n",
    "        \n",
    "        # Add legend\n",
    "        ax.legend()\n",
    "\n",
    "        # Save the plot to the PDF\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Print out results\n",
    "        print(f\"Results for {target}:\")\n",
    "        print(f\"  R² on full dataset: {r2:.4f}\")\n",
    "\n",
    "print(f\"All plots have been saved to {pdf_filename}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "# Load the CSV data\n",
    "file_path = 'NWCA2011S.csv'  # Replace with the path to your CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract LAT_ANALYS and LON_ANALYS columns\n",
    "latitude = data['LAT_ANALYS']\n",
    "longitude = data['LON_ANALYS']\n",
    "\n",
    "# Initialize the map centered on the U.S. (around latitude 37.0902, longitude -95.7129)\n",
    "us_map = folium.Map(location=[37.0902, -95.7129], zoom_start=5)\n",
    "\n",
    "# Add each point from the CSV file to the map as a small red dot\n",
    "for lat, lon in zip(latitude, longitude):\n",
    "    folium.CircleMarker(\n",
    "        location=[lat, lon],\n",
    "        radius=2,  # Small radius for the dot\n",
    "        color='red',  # Outline color\n",
    "        fill=True,\n",
    "        fill_color='red',  # Fill color\n",
    "        fill_opacity=1.0\n",
    "    ).add_to(us_map)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "us_map.save('us_map_2011.html')\n",
    "\n",
    "# Display the map\n",
    "us_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "# Load the CSV data\n",
    "file_path = 'NWCA2016S.csv'  # Replace with the path to your CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract LAT_ANALYS and LON_ANALYS columns\n",
    "latitude = data['LAT_ANALYS']\n",
    "longitude = data['LON_ANALYS']\n",
    "\n",
    "# Initialize the map centered on the U.S. (around latitude 37.0902, longitude -95.7129)\n",
    "us_map = folium.Map(location=[37.0902, -95.7129], zoom_start=5)\n",
    "\n",
    "# Add each point from the CSV file to the map as a small red dot\n",
    "for lat, lon in zip(latitude, longitude):\n",
    "    folium.CircleMarker(\n",
    "        location=[lat, lon],\n",
    "        radius=2,  # Small radius for the dot\n",
    "        color='red',  # Outline color\n",
    "        fill=True,\n",
    "        fill_color='red',  # Fill color\n",
    "        fill_opacity=1.0\n",
    "    ).add_to(us_map)\n",
    "    \n",
    "# Save the map to an HTML file\n",
    "us_map.save('us_map_2016.html')\n",
    "\n",
    "# Display the map\n",
    "us_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "# Load the CSV data\n",
    "file_path = 'NWCA2021S.csv'  # Replace with the path to your CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract LAT_ANALYS and LON_ANALYS columns\n",
    "latitude = data['LAT_ANALYS']\n",
    "longitude = data['LON_ANALYS']\n",
    "\n",
    "# Initialize the map centered on the U.S. (around latitude 37.0902, longitude -95.7129)\n",
    "us_map = folium.Map(location=[37.0902, -95.7129], zoom_start=5)\n",
    "\n",
    "# Add each point from the CSV file to the map as a small red dot\n",
    "for lat, lon in zip(latitude, longitude):\n",
    "    folium.CircleMarker(\n",
    "        location=[lat, lon],\n",
    "        radius=2,  # Small radius for the dot\n",
    "        color='red',  # Outline color\n",
    "        fill=True,\n",
    "        fill_color='red',  # Fill color\n",
    "        fill_opacity=1.0\n",
    "    ).add_to(us_map)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "us_map.save('us_map_2021.html')\n",
    "\n",
    "# Display the map\n",
    "us_map\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
